# Awesome-Model-Compression-in-Recommender-Systems

A collection of high-quality(newest) papers of model compression, particularly in the field of recommender systems. Due to the discreteness of input data and the characteristics of massive users and items in recommendation scenarios, the compression technics or  implementations are quite different from those commonly used in Computer Vision tasks. And this is the why this repro is created.
----
## Feature

> - focus on compression skills on RS and NLP
> - provide two rules to ca
----

## Parameters Pruning

1.  Yang Sun et al., “A Generic Network Compression Framework for Sequential Recommender Systems” (arXiv, May 26, 2020), [http://arxiv.org/abs/2004.13139](http://arxiv.org/abs/2004.13139).
2.  Fei Mi, Xiaoyu Lin, and Boi Faltings, “ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning for Session-Based Recommendation,” 2020, 6, [https://doi.org/10.1145/3383313.3412218](https://doi.org/10.1145/3383313.3412218).
3.  Wonbin Kweon, SeongKu Kang, and Hwanjo Yu, “Bidirectional Distillation for Top-K Recommender System,” June 5, 2021, [https://doi.org/10.1145/3442381.3449878](https://doi.org/10.1145/3442381.3449878).
4.  Jae-woong Lee et al., “Collaborative Distillation for Top-N Recommendation” (arXiv, November 12, 2019), [http://arxiv.org/abs/1911.05276](http://arxiv.org/abs/1911.05276).
5.  Hao-Jun Michael Shi et al., “Compositional Embeddings Using Complementary Partitions for Memory-Efficient Recommendation Systems,” June 28, 2020, [https://doi.org/10.1145/3394486.3403059](https://doi.org/10.1145/3394486.3403059).
6.  SeongKu Kang et al., “DE-RRD: A Knowledge Distillation Framework  for Recommender System,” 2020, 10, [https://doi.org/10.1145/3340531.3412005](https://doi.org/10.1145/3340531.3412005).
7.  Wei Deng et al., “DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving” (arXiv, January 6, 2021), [http://arxiv.org/abs/2002.06987](http://arxiv.org/abs/2002.06987).
8.  Defu Lian et al., “Defu.LightRec:,” *WWW*, 2020, 11.
9.  Shitao Xiao et al., “Distill-VQ: Learning Retrieval Oriented Vector Quantization By Distilling Knowledge from Dense Embeddings” (arXiv, April 28, 2022), [http://arxiv.org/abs/2204.00185](http://arxiv.org/abs/2204.00185).
10.  Jieming Zhu et al., “Ensembled CTR Prediction via Knowledge Distillation,” in *Proceedings of the 29th ACM International Conference on Information & Knowledge Management* (CIKM ’20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event Ireland: ACM, 2020), 2941–58, [https://doi.org/10.1145/3340531.3412704](https://doi.org/10.1145/3340531.3412704).
11.  Jun-Gi Jang and U Kang, “Fast and Memory-Efficient Tucker Decomposition for Answering Diverse Time Range Queries,” in *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining* (KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event Singapore: ACM, 2021), 725–35, [https://doi.org/10.1145/3447548.3467290](https://doi.org/10.1145/3447548.3467290).
12.  Lin Wu et al., “Few-Shot Deep Adversarial Learning for Video-Based Person Re-Identification,” *IEEE Transactions on Image Processing* 29 (2020): 1233–45, [https://doi.org/10.1109/TIP.2019.2940684](https://doi.org/10.1109/TIP.2019.2940684).
13.  Amina Kammoun et al., “Generative Adversarial Networks for Face Generation: A Survey,” *ACM Computing Surveys*, March 31, 2022, 1122445.1122456, [https://doi.org/10.1145/1122445.1122456](https://doi.org/10.1145/1122445.1122456).
14.  Siyi Liu et al., “Learnable Embedding Sizes for Recommender Systems” (arXiv, March 11, 2021), [http://arxiv.org/abs/2101.07577](http://arxiv.org/abs/2101.07577).
15.  Yankai Chen et al., “Learning Binarized Graph Representations with Multi-Faceted Quantization Reinforcement for Top-K Recommendation” (arXiv, June 5, 2022), [http://arxiv.org/abs/2206.02115](http://arxiv.org/abs/2206.02115).
16.  Tong Chen et al., “Learning Elastic Embeddings for Customizing On-Device Recommenders” (arXiv, June 3, 2021), [http://arxiv.org/abs/2106.02223](http://arxiv.org/abs/2106.02223).
17.  Wang-Cheng Kang et al., “Learning Multi-Granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems,” in *Companion Proceedings of the Web Conference 2020* (WWW ’20: The Web Conference 2020, Taipei Taiwan: ACM, 2020), 562–66, [https://doi.org/10.1145/3366424.3383416](https://doi.org/10.1145/3366424.3383416).
18.  Antonio Ginart et al., “Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems” (arXiv, February 8, 2021), [http://arxiv.org/abs/1909.11810](http://arxiv.org/abs/1909.11810).
19.  Chuhan Wu et al., “NewsBERT: Distilling Pre-Trained Language Model for Intelligent News Application” (arXiv, September 2, 2021), [http://arxiv.org/abs/2102.04887](http://arxiv.org/abs/2102.04887).
20.  Qinyong Wang et al., “Next Point-of-Interest Recommendation on Resource-Constrained Mobile Devices,” *WWW*, 2020, 11.
21.  Xin Xia et al., “On-Device Next-Item Recommendation with Self-Supervised Knowledge Distillation” (arXiv, April 23, 2022), [http://arxiv.org/abs/2204.11091](http://arxiv.org/abs/2204.11091).
22.  Fuyuan Lyu et al., “OptEmbed: Learning Optimal Embedding Table for Click-through Rate Prediction,” September 6, 2022, [https://doi.org/10.1145/3511808.3557411](https://doi.org/10.1145/3511808.3557411).
23.  Tiezheng Ge et al., “Optimized Product Quantization for Approximate Nearest Neighbor Search,” in *2013 IEEE Conference on Computer Vision and Pattern Recognition* (2013 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Portland, OR, USA: IEEE, 2013), 2946–53, [https://doi.org/10.1109/CVPR.2013.379](https://doi.org/10.1109/CVPR.2013.379).
24.  Chen Xu et al., “Privileged Features Distillation at Taobao Recommendations” (arXiv, February 25, 2020), [http://arxiv.org/abs/1907.05171](http://arxiv.org/abs/1907.05171).
25.  Herve Jegou, Matthijs Douze, and Cordelia Schmid, “Product Quantization for Nearest Neighbor Search,” n.d., 14, [https://doi.org/10.1109/TPAMI.2010.57](https://doi.org/10.1109/TPAMI.2010.57).
26.  Defu Lian et al., “Product Quantized Collaborative Filtering,” *IEEE Transactions on Knowledge and Data Engineering* 33, no. 9 (September 1, 2021): 3284–96, [https://doi.org/10.1109/TKDE.2020.2964232](https://doi.org/10.1109/TKDE.2020.2964232).
27.  Jiaxi Tang and Ke Wang, “Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System,” *KDD*, 2018, 10, [https://doi.org/10.1145/3219819.3220021](https://doi.org/10.1145/3219819.3220021).
28.  Guorui Zhou et al., “Rocket Launching: A Universal and Efficient Framework for Training Well-Performing Light Net” (arXiv, March 14, 2018), [http://arxiv.org/abs/1708.04106](http://arxiv.org/abs/1708.04106).
29.  Xiaorui Wu et al., “Saec: Similarity-Aware Embedding Compression in Recommendation Systems” (arXiv, February 26, 2019), [http://arxiv.org/abs/1903.00103](http://arxiv.org/abs/1903.00103).
30.  Liang Qu et al., “Single-Shot Embedding Dimension Search in Recommender System” (arXiv, April 14, 2022), [http://arxiv.org/abs/2204.03281](http://arxiv.org/abs/2204.03281).
31.  Yankai Chen et al., “Towards Low-Loss 1-Bit Quantization of User-Item Representations for Top-K Recommendation” (arXiv, December 3, 2021), [http://arxiv.org/abs/2112.01944](http://arxiv.org/abs/2112.01944).
32.  Ahmad Rashid et al., “Towards Zero-Shot Knowledge Distillation for Natural Language Processing” (arXiv, December 31, 2020), [http://arxiv.org/abs/2012.15495](http://arxiv.org/abs/2012.15495).
33.  Chunxing Yin et al., “TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models” (arXiv, January 25, 2021), [http://arxiv.org/abs/2101.11714](http://arxiv.org/abs/2101.11714).
34.  Jiayi Shen et al., “UMEC: UNIFIED MODEL AND EMBEDDING COMPRES- SION FOR EFFICIENT RECOMMENDATION SYSTEMS,” 2021, 13.
35.  Gangwei Jiang et al., “XLightFM: Extremely Memory-Efficient Factorization Machine,” in *Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval* (SIGIR ’21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event Canada: ACM, 2021), 337–46, [https://doi.org/10.1145/3404835.3462941](https://doi.org/10.1145/3404835.3462941).
## Quantization

## Low-Rank Factorization

## Distillation 


This repo aims to provide the info for model compression research (especially in RS and NLP). Welcome to PR the works (papers, repositories) that are missed by the repo.
